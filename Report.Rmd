---
title: "The Italian Spring"
subtitle: " The Italian institutional crisis, a Twitter perspective "
author: "Domagoj Korais"
date: " Spring 2018"
output:
  html_document:
    toc: yes
  ioslides_presentation:
    highlight: tango
  include: null
  beamer_presentation:
    highlight: tango
  pdf_document:
    highlight: tango
    keep_tex: yes
    toc: yes
  slide_level: 2
  slidy_presentation:
    fig.height: 6
    fig.width: 8
    highlight: tango
header-includes:
- \usepackage{color}
- \definecolor{Purple}{HTML}{911146}
- \definecolor{Orange}{HTML}{CF4A30}
- \setbeamercolor{alerted text}{fg=Orange}
- \setbeamercolor{frametitle}{bg=Purple}
institute: University of Udine & University of Trieste
graphics: yes
fontsize: 10pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE )
knitr::opts_chunk$set(cache = TRUE )
knitr::opts_chunk$set(eval = TRUE )
knitr::opts_chunk$set(warning = FALSE )
knitr::opts_chunk$set(message = FALSE )
```


##Motivation and overview of the project.
The rise of populism in western democratic countries is one of the characteristic trademarks of our time.
According to various researches [1] social networks played a pivotal role in the spreading of information, and misinformation, during all the major events of the last couple of years, from Trump's victory in the USA elections to the Brexit referendum in UK.
May 27th was a very peculiar day for Italian institutions:  President Sergio Mattarella vetoed Paolo Savona as finance minister, triggering an institutional crisis, that culminated in the proposal of impeachment made by Luigi Di Maio, leader of the M5S party.
In the present project I collect and analyze a stream of tweets linked with this crisis, with the aim of studying the opinion of Italian Twitter users regarding those events.
The project is divided in three main parts: data collection & cleaning, tweets analysis and retweets analysis.
Two interactive applications has been developed with the aim to build an interactive tweet analysis framework.
I believe that text mining places itself at the boundary between automatic and human-made knowledge extraction, so the project is oriented towards the presentation of the data in such a way to help comprehension to a person with some knowledge about the topic discussed.




# Data collection and cleaning

##Data collection
Data collection has been performed in the wake of the crisis, since free twitter APIs permits to recover tweets only in a time span of 6-9 days before the request time. 
Data has been gathered using the "Rtweet" package searching for the following hashtags and keywords: #iostoconmattarella, #dimaio, #salvini, #ilmiovotoconta, #governolegam5s, #impeachment, #impeachmentmattarella, #iostoconmattarella, #mattarella, #mattarelladimettiti, #oettinger, #spread, "mattarella","spread".
The total number of gathered tweets is **1012909**, for the period from 23-05-2018 to 03-06-2018.
The total number of different users whose metadata has been collected is **103037**.  
The total number of associated metadata is 40 for each tweet, and 19 for each user. 

 
 
##Data cleaning 

Data cleaning has been driven bearing in mind the principles of tidy data, both for textual and metadata parts.
Since many tweets has been collected multiple times, the first step is to get rid of duplicates. After that operation the number of single tweets dropped from **1012909** to **764625**, of which **519467 ** retweets and **245158** tweets.
The second step is to retain only Italian tweets, since I'm interested in Italians opinions. The filtering is trivial since with every tweet come an associated metadata tag indicating the language. This operation diminished the number of tweets to **695648** of which **485887** retweets and **209761** tweets.
Text cleaning has been performed using regular expressions to clean up both the texts and the hashtags from undesired values such us urls, quotation marks and other undesired elements.
One of the major problems solved in the cleaning phase is that, since Twitter changed the maximum size of tweets from 140 to 280, the retweets text was very often truncated, and some of the hashtags were missing. 
The problem has been solved searching the text of the original retweeted tweet, and extracting the informations from there.
The Italian stop words were downloaded from a Github project [2], and were used to remove extremely common Italian words, that doesn't provide useful informations.








#Tweets analysis

## Tweets localization
In this visualization is possible to take a first look on the distribution of tweets around the world.
In the interactive version of the report this image is interactive, here I present only a screenshot.
Only the subset of geotagged tweets is shown.
![Tweets distribution](/home/doma/0dssc/data_managment/esame_finale/tweets.png). 













##Posting frequency
Let's explore the posting frequency, for tweets and retweets.
In those graphs I plotted the posting frequency on different timescales, the red area shows the period of time in which official declarations were released from the authorities, the exact timestamp of them was extracted from the official Twitter account of the "Quirinale".
There are two main characteristics: the day-night modulation, with the majority of tweets posted during the day and only a small minority on night, and two sharp spikes on May 27 and June 01, corresponding respectively to the collapse of the forming government and to it's final formation. 
For the ratio between tweets and retweets see the disclaimer in the retweet section.



\thinspace
\thinspace

```{r "Posting frequency"}
library(rtweet)
library(leaflet)
library(tidyverse)

dataset_untidy = readRDS( "/home/doma/0dssc/data_managment/esame_finale/data/FINAL_DATA_DEFINITIVO/tutti_i_tweet_no_duplicati.RDS" )
#only italian language tweets
dataset_untidy = dataset_untidy %>% filter ( lang == "it" )

#start and finish of declarations 27th of May from Mattarella and co.
TIMEFRAME=data.frame(begin=as.POSIXct("2018-05-27 17:15:21", tz = "GMT"),end=as.POSIXct("2018-05-27 18:57:37", tz = "GMT"))

 dataset_untidy %>%
  group_by ( is_retweet ) %>%
  filter ( created_at >= "2018-05-23" & created_at <= "2018-06-03" ) %>%
  ts_plot ( "1 hour" ) +
  ggplot2::theme_minimal () +
  ggplot2::theme ( plot.title = ggplot2::element_text ( face = "bold" ) ) +
  ggplot2::labs(
    x = NULL , y = NULL ,
    title = "Frequency  Tweets statuses for period 23/05/2018-03/06/2018" ,
    subtitle = "Twitter status (tweet) counts aggregated using one-hour intervals" ,
    caption = "\nSource: Data collected from Twitter's REST API via rtweet" ,
    color = "Retweet?" )  +
    geom_rect ( data = TIMEFRAME,
                aes(xmin = as.POSIXct(begin,tz = "GMT"), xmax =as.POSIXct( end,tz = "GMT"), ymin = -Inf, ymax = +Inf),
                inherit.aes = FALSE, fill = "red", alpha = 0.3)



    





```

\thinspace
\thinspace

Let's take a closer look on the first spike:
The relation between official declarations and twitter activity is clear.

\thinspace
\thinspace

```{r "Posting frequency-limitated timespan"}
library(rtweet)
library(leaflet)
library(tidyverse)

dataset_untidy = readRDS( "/home/doma/0dssc/data_managment/esame_finale/data/FINAL_DATA_DEFINITIVO/tutti_i_tweet_no_duplicati.RDS" )
#only italian language tweets
dataset_untidy = dataset_untidy %>% filter ( lang == "it" )

#start:Mattarella riceve Conte al Quirinale, end: Dichiarazione alla stampa di Mattarella, fonte: account Quirinale  twitter
TIMEFRAME=data.frame(begin=as.POSIXct("2018-05-27 17:15:21", tz = "GMT"),end=as.POSIXct("2018-05-27 18:57:37", tz = "GMT"))
#limit timespan
 dataset_untidy %>%
 group_by ( is_retweet ) %>%
 filter ( created_at >= as.POSIXct("2018-05-27 13:22:13",tz = "GMT") & created_at <= as.POSIXct("2018-05-27 23:32:13",tz = "GMT") ) %>%
  #filter ( created_at >="2018-05-27" & created_at <="2018-05-27") %>%
  ts_plot ( "1 minute" ) +
  ggplot2::theme_minimal ( ) +
  ggplot2::theme ( plot.title = ggplot2::element_text ( face = "bold" ) ) +
  ggplot2::labs (
    x = NULL, y = NULL,
    title = "Frequency  Tweets statuses for period 13:22-23:32 27 May 2018",
    subtitle = "Twitter status (tweet) counts aggregated using one minute intervals",
    caption = "\nSource: Data collected from Twitter's REST API via rtweet",
    color = "Retweet?" ) +
    geom_rect ( data = TIMEFRAME,
                aes(xmin = as.POSIXct(begin,tz = "GMT"), xmax =as.POSIXct( end,tz = "GMT"), ymin = -Inf, ymax = +Inf),
                inherit.aes = FALSE, fill = "red", alpha = 0.1)


```

##Most common hashtags 

Now that we are sure about the correlation between political happenings and tweets activity, let's start our data exploration from the hashtags.
Plotting the 20 top hashtags gives us an idea of what other hashtags has been used apart the ones collected directly, and it's a first check for data quality. In fact the same hashtag can be written in a lot of different manners, and the fact that there are not similar duplicates is a proof of work of the cleaning pipeline.

```{r "top 20 Hashtag analysis"}
#hashtag study

#tidying dataset for lexical analysis
library(tidyverse)
library(rtweet)

library(gridExtra)

library(tidytext)
library(widyr)
library(igraph)
library(ggraph)
#--------------------------------------------------------------------------------------------Hashtag
#Tidy for hashtag study
dataset=readRDS("/home/doma/0dssc/data_managment/esame_finale/data/FINAL_DATA_DEFINITIVO/reduced_tweet_no_duplicati_stripped_text_separated.RDS")
#only italian language tweets
dataset=dataset%>%filter(lang=="it")





# remove punctuation, convert to lowercase.
dataset_clean <- dataset %>%
  select(hashtags) %>%
  unnest_tokens(word, hashtags)
#put in original dataframe
dataset$hashtags=dataset_clean$word

a=dataset_clean %>%
  count(word, sort = TRUE)




#plot the top 20 HASHTAGS
dataset_clean %>%
  count(word, sort = TRUE) %>%
  
  top_n(20+1) %>%
  filter(!is.na(word))%>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x = word, y = n)) +
  geom_col(aes(alpha=n),show.legend =FALSE) +
  xlab(NULL) +
  coord_flip() +
  labs(x = "Hashtags",
       y = "Count",
       title = " Top 20 hashtags by count")+
  ggplot2::theme_minimal()

```



##Hashtags frequency
Here it's possible to see how the hashtag composition changed over time.
I selected from all the hashtags the ones related to major events separated in time, otherwise the plot becomes unreadable.
As we can see the hashtag "Mattarella" is very common, and that's reasonable since it's the main topic of all the discussion.
On May 28th there is a spike in hashtag "ilmiovotoconta", in fact that's the moment when Luigi Di Maio launched it from his account.
On the First of June it's possible to see the spike related to the new government [3],
signaled by the spike in hashtag "governolegam5s".



```{r "Hashtag analysis"}

#hashtag study

#tidying dataset for lexical analysis
library(tidyverse)
library(rtweet)

library(gridExtra)

library(tidytext)
library(widyr)
library(igraph)
library(ggraph)
#--------------------------------------------------------------------------------------------Hashtag
#Tidy for hashtag study
dataset=readRDS("/home/doma/0dssc/data_managment/esame_finale/data/FINAL_DATA_DEFINITIVO/reduced_tweet_no_duplicati_stripped_text_separated.RDS")
#only italian language tweets
dataset=dataset%>%filter(lang=="it")





# remove punctuation, convert to lowercase.
dataset_clean <- dataset %>%
  select(hashtags) %>%
  unnest_tokens(word, hashtags)
#put in original dataframe
dataset$hashtags=dataset_clean$word

#POSTING ANALYSIS DIVIDED BY TWEET

top_hashtags=dataset_clean %>%
  count(word, sort = TRUE) %>%
  top_n(10)%>%
  filter(!is.na(word))

#Operazione fondamentale per beccare le parole esatte! altrimenti la regex ne becca troppe (semepio mattarella ----> iostoconmattarella)
top_hashtags$word= sub("^", "^", top_hashtags$word )
top_hashtags$word= sub("$", "$", top_hashtags$word )

#filtro il dataset
dataset=dataset %>% mutate(has_top_hashtag=str_detect(hashtags,paste(top_hashtags$word,collapse= '|')))

#all data series
dataset%>%
  filter(hashtags=="mattarella" | hashtags == "iostoconmattarella" | hashtags == "ilmiovotoconta" | hashtags == "governolegam5s")%>%
  group_by(hashtags,is_retweet)%>%
  filter(created_at>="2018-05-23"& created_at<="2018-06-03") %>%
  ts_plot( "1 hour") +
  ggplot2::theme_minimal() +
  ggplot2::theme(plot.title = ggplot2::element_text(face = "bold")) +
  ggplot2::labs(
    x = NULL,
    y = NULL,
    title = "Frequency  Tweets statuses for period 23/05/2018-03/06/2018",
    subtitle = "Twitter status (tweet) counts aggregated using one-hour intervals",
    caption = "\nSource: Data collected from Twitter's REST API via rtweet",
    linetype=" Retweet?",
    colour = " Hashtags"
  )  





```

Let's look again at the first spike,this time the selected hashtags are the ones depicting some kind of feeling, namely support or aversion to the President's decision. However this interpretation is to take "cum grano salis" since on twitter hashtags are ofthen used with irony, so the correlation between the hashtag and it's meaning is not so immediate.
Again the red square mark the official communications period from the authorities.
As we can see  the pattern for the two main tweets is quite similar, more retweets than tweets.
The absolute numbers for the peak period are similar too, even if the tails are quite different, the hashtag "iostoconmattarella" has a longer tail, whereas "mattarelladimettiti" has a steeper pattern.

```{r "Hashtag analysis zoom"}

#hashtag study

#tidying dataset for lexical analysis
library(tidyverse)
library(rtweet)

library(gridExtra)

library(tidytext)
library(widyr)
library(igraph)
library(ggraph)
#--------------------------------------------------------------------------------------------Hashtag
#Tidy for hashtag study
dataset=readRDS("/home/doma/0dssc/data_managment/esame_finale/data/FINAL_DATA_DEFINITIVO/reduced_tweet_no_duplicati_stripped_text_separated.RDS")
#only italian language tweets
dataset=dataset%>%filter(lang=="it")

#zoom on hot time
p4=dataset%>%
  #filter(has_top_hashtag==TRUE)%>%
    filter(hashtags=="iostoconmattarella" | hashtags=="mattarelladimettiti" | hashtags=="impeachment" 
           | hashtags=="impeachmentmattarella" )%>%

  group_by(hashtags,is_retweet)%>%
   filter ( created_at >= as.POSIXct("2018-05-27 16:00:00",tz = "GMT") & created_at <= as.POSIXct("2018-05-28 00:00:00",tz = "GMT") ) %>%
  ts_plot( "5 minutes") +
  ggplot2::theme_minimal() +
  ggplot2::theme(plot.title = ggplot2::element_text(face = "bold")) +
  ggplot2::labs(
    x = NULL,
    y = NULL,
    title = "Frequency  Tweets statuses for period 16:00-24:00 27/05/2018",
    subtitle = "Twitter status (tweet) counts aggregated using five-minutes intervals",
    caption = "\nSource: Data collected from Twitter's REST API via rtweet",
    linetype="Retweet?",
    colour = " Hashtags"
  )  +
    geom_rect(data = TIMEFRAME,
                aes(xmin = as.POSIXct(begin,tz = "GMT"), xmax =as.POSIXct( end,tz = "GMT"), ymin = -Inf, ymax = +Inf),
                inherit.aes = FALSE, fill = "red", alpha = 0.1)
                
                
                p4
```



## Words count 



Since the goal of this part is to characterize the users by the words they used I retained only non duplicated non retweets, otherwise the results would be artificially inflated since the words presents in a very popular retweets would be overwhelming on the others.
Additionally the tweets has been stripped of numbers, links, hashtags, with the exception of the number 5, that is in the name of the 5 Star Movement, so it shouldn't be removed for a proper analysis.
Again the word count shows that the cleaning pipeline is working and we don't have improper words. All the words in the top 20 list are linked with politics, politicians or political parties.
At this point we are still unable to to grasp the overall people sentiment, and dividing the words by their associated hashtags doesn't help since the majority of words are the same.

```{r "word tidy analysis",warning=FALSE,message=FALSE}
library(gridExtra)
library(tidyverse)
library(tidytext)
library(widyr)
library(rtweet)


# load list of stop words - from github project
italian_stop_words=read.csv("https://raw.githubusercontent.com/stopwords-iso/stopwords-it/master/stopwords-it.txt")
colnames(italian_stop_words)="word"
italian_stop_words$word=as.character(italian_stop_words$word)
#modifico aggiungendo una "a"
italian_stop_words=rbind(italian_stop_words,"a")



#proviamo a dividire per hashtag, qui correggo e metto gli hashtag giusti senza maiuscole e problemi vari
unique_tweets=readRDS("/home/doma/0dssc/data_managment/esame_finale/data/FINAL_DATA_DEFINITIVO/all_tweets_cleaned_stripped_unique_ita.rds")
unique_tweets_tidy0=separate_rows(unique_tweets) 
dataset_clean <- unique_tweets_tidy0 %>%
  select(hashtags) %>%
  unnest_tokens(word, hashtags)
unique_tweets_tidy0$hashtags=dataset_clean$word

#in ordine di apparizione
hashes= c(  "mattarella", "salvini","dimaio","iostoconmattarella", "ilmiovotoconta","governolegam5s","savona","mattarelladimettiti","cottarelli","impeachment","impeachmentmattarella","maratonamentana") 




#------------------------------------------blocco 1 "iostoconmattarella"
#filtro per hashtag
unique_tweets_tidy=unique_tweets_tidy0%>%
  filter(hashtags=="iostoconmattarella")

#----------------------------------------------------------pezzo che si occupa di calcolare la frequenza delle parole
#contiamo le parole, non gli hashtag
dataset_clean <- unique_tweets_tidy %>%
  select(stripped_text) %>%
  unnest_tokens(word, stripped_text)
# remove stop words from your list of words
dataset_clean <- dataset_clean %>%
  anti_join(italian_stop_words)
#-----------------------------------------------------------

# #plot the top 20 words
plot1=dataset_clean %>%
  count(word, sort = TRUE) %>%
  top_n(20) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x = word, y = n)) +
  geom_col(aes(alpha=n),show.legend =FALSE) +
  xlab(NULL) +
  coord_flip() +
  labs(x = "Unique words",
       y = "Count",
       title = paste("#" ,"iostoconmattarella"))+
  theme_minimal()


#------------------------------------------


#------------------------------------------blocco 2 contro mattarella
#filtro per hashtag
unique_tweets_tidy=unique_tweets_tidy0%>%
  filter(hashtags=="impeachment" | hashtags=="impeachmentmattarella" | hashtags=="mattarelladimettiti")

#----------------------------------------------------------pezzo che si occupa di calcolare la frequenza delle parole
#contiamo le parole, non gli hashtag
dataset_clean <- unique_tweets_tidy %>%
  select(stripped_text) %>%
  unnest_tokens(word, stripped_text)
# remove stop words from your list of words
dataset_clean <- dataset_clean %>%
  anti_join(italian_stop_words)
#-----------------------------------------------------------

# #plot the top 20 words
plot2=dataset_clean %>%
  count(word, sort = TRUE) %>%
  top_n(20) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x = word, y = n)) +
  geom_col(aes(alpha=n),show.legend =FALSE) +
  xlab(NULL) +
  coord_flip() +
  labs(x = "Unique words",
       y = "Count",
       title = paste("#" ,"mattarelladimmetti +"),
  subtitle="#impeachment + #impeachmentmattarella")+
  theme_minimal()


#------------------------------------------

#------------------------------------------blocco 3 mattarella
#filtro per hashtag
unique_tweets_tidy=unique_tweets_tidy0%>%
  filter(hashtags=="mattarella" )

#----------------------------------------------------------pezzo che si occupa di calcolare la frequenza delle parole
#contiamo le parole, non gli hashtag
dataset_clean <- unique_tweets_tidy %>%
  select(stripped_text) %>%
  unnest_tokens(word, stripped_text)
# remove stop words from your list of words
dataset_clean <- dataset_clean %>%
  anti_join(italian_stop_words)
#-----------------------------------------------------------

# #plot the top 20 words
plot3=dataset_clean %>%
  count(word, sort = TRUE) %>%
  top_n(20) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x = word, y = n)) +
  geom_col(aes(alpha=n),show.legend =FALSE) +
  xlab(NULL) +
  coord_flip() +
  labs(x = "Unique words",
       y = "Count",
       title = paste("#" ,"mattarella"))+
  theme_minimal()


#------------------------------------------

#------------------------------------------blocco 4 salvini
#filtro per hashtag
unique_tweets_tidy=unique_tweets_tidy0%>%
  filter(hashtags=="salvini" )

#----------------------------------------------------------pezzo che si occupa di calcolare la frequenza delle parole
#contiamo le parole, non gli hashtag
dataset_clean <- unique_tweets_tidy %>%
  select(stripped_text) %>%
  unnest_tokens(word, stripped_text)
# remove stop words from your list of words
dataset_clean <- dataset_clean %>%
  anti_join(italian_stop_words)
#-----------------------------------------------------------

# #plot the top 20 words
plot4=dataset_clean %>%
  count(word, sort = TRUE) %>%
  top_n(20) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x = word, y = n)) +
  geom_col(aes(alpha=n),show.legend =FALSE) +
  xlab(NULL) +
  coord_flip() +
  labs(x = "Unique words",
       y = "Count",
       title = "#salvini")+
  theme_minimal()


#------------------------------------------

#------------------------------------------blocco 5 di maio
#filtro per hashtag
unique_tweets_tidy=unique_tweets_tidy0%>%
  filter(hashtags=="dimaio" )

#----------------------------------------------------------pezzo che si occupa di calcolare la frequenza delle parole
#contiamo le parole, non gli hashtag
dataset_clean <- unique_tweets_tidy %>%
  select(stripped_text) %>%
  unnest_tokens(word, stripped_text)
# remove stop words from your list of words
dataset_clean <- dataset_clean %>%
  anti_join(italian_stop_words)
#-----------------------------------------------------------

# #plot the top 20 words
plot5=dataset_clean %>%
  count(word, sort = TRUE) %>%
  top_n(20) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x = word, y = n)) +
  geom_col(aes(alpha=n),show.legend =FALSE) +
  xlab(NULL) +
  coord_flip() +
  labs(x = "Unique words",
       y = "Count",
       title = "#dimaio")+
  theme_minimal()


#------------------------------------------

#------------------------------------------blocco 6 tutti assieme
#filtro per hashtag
unique_tweets_tidy=unique_tweets_tidy0
  

#----------------------------------------------------------pezzo che si occupa di calcolare la frequenza delle parole
#contiamo le parole, non gli hashtag
dataset_clean <- unique_tweets_tidy %>%
  select(stripped_text) %>%
  unnest_tokens(word, stripped_text)
# remove stop words from your list of words
dataset_clean <- dataset_clean %>%
  anti_join(italian_stop_words)
#-----------------------------------------------------------

# #plot the top 20 words
plot6=dataset_clean %>%
  count(word, sort = TRUE) %>%
  top_n(20) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x = word, y = n)) +
  geom_col(aes(alpha=n),show.legend =FALSE) +
  xlab(NULL) +
  coord_flip() +
  labs(x = "Unique words",
       y = "Count",
       title = "all tweets")+
  theme_minimal()


#------------------------------------------


# #plotting results

 # grid.arrange(plot1,plot2,ncol=2)
 # grid.arrange(plot3,plot6,ncol=2)
 # grid.arrange(plot5,plot4,ncol=2)

plot(plot6)







```

##Words are important
Here comes the central part of this work. It presents some kind of novelty as compared to works about sentiment analysis of Twitter data that I read about. The idea is to reveal users sentiment using their vocabulary filtered using the vocabulary of the group that opposes their ideas, but not all of it, only a subset, otherwise we remove too much information.
In practice what I do is to divide the tweets by hashtag, then count the most occurring words. At this point for each hashtag, or group of hashtags, I retain only the words with a frequency higher than a certain threshold, the value of which has to be estimated experimentally.
Finally I subtract all the common words between the two groups and what is left is the characteristic words of the underlying population.
Additionally that's the only way that I found to tackle the problems introduced by two extremely penalizing conditions: the language of tweets and the extremely peculiar lexicon used. The first is a problem because the majority of work on sentiment analysis is done for the English language, and I don't want to to delve into the realm of unsupervised learning. The second is a problem because there aren't available precompiled lists of words to use to perform sentIment analysis in the classical way.


I developed an interactive application to perform this operation choosing the hashtags, and the minimum number of words to use as a filter.
This application, tohether with the other presented in the next chapter, compose the interactive tweets analysis framework that i talked about in the introduction. I think that is a useful tool to grasp an idea about users sentiment.
They can easily be modified to study any kind of tweets dataset where there are a polarized discussion on a topic, like in political situations, elections, referendums end so on.

Link for interactive application: https://offlagadiscopax.shinyapps.io/lexicons/

In the following graphs is possible to see the results of this operation performed on various subsets.
```{r,"Subgroups words usage mattarella vs tutti"}

library("gridExtra")
library(tidyverse)
library(tidytext)
library(widyr)
library(igraph)
library(ggraph)


# load list of stop words - from github project
italian_stop_words=read.csv("https://raw.githubusercontent.com/stopwords-iso/stopwords-it/master/stopwords-it.txt")
colnames(italian_stop_words)="word"
italian_stop_words$word=as.character(italian_stop_words$word)
#modifico aggiungendo una "a"
italian_stop_words=rbind(italian_stop_words,"a")


#proviamo a dividire per hashtag, qui correggo e metto gli hashtag giusti senza maiuscole e problemi vari
unique_tweets=readRDS("/home/doma/0dssc/data_managment/esame_finale/data/FINAL_DATA_DEFINITIVO/all_tweets_cleaned_stripped_unique_ita.rds")
unique_tweets_tidy0=separate_rows(unique_tweets) 
dataset_clean <- unique_tweets_tidy0 %>%
  select(hashtags) %>%
  unnest_tokens(word, hashtags)
unique_tweets_tidy0$hashtags=dataset_clean$word

#in ordine di apparizione
hashes= c(  "mattarella", "salvini","dimaio","iostoconmattarella", "ilmiovotoconta","governolegam5s","savona","mattarelladimettiti","cottarelli","impeachment","impeachmentmattarella","maratonamentana") 

#for unique words
datalist = list()
i=1
for(i in 1:length(hashes)){
hash=hashes[i]

#filtro per hashtag
unique_tweets_tidy=unique_tweets_tidy0%>%
  filter(hashtags==hash)
#----------------------------------------------------------pezzo che si occupa di calcolare la frequenza delle parole
#contiamo le parole, non gli hashtag
dataset_clean <- unique_tweets_tidy %>%
  select(stripped_text) %>%
  unnest_tokens(word, stripped_text)
# remove stop words from your list of words
dataset_clean <- dataset_clean %>%
  anti_join(italian_stop_words)
#-----------------------------------------------------------
all_words_counted=dataset_clean %>%
  count(word, sort = TRUE)
all_words_counted=mutate(all_words_counted,hashtags=hash)
#store data in a list to open later
datalist[[i]]=all_words_counted
}
#binding results of counting words
counted_words = do.call(rbind, datalist)



#and now a comparison of the vocabulary used by the subgroups
#----------------------------------------
#Common and special lexicons
minimum_repetitions=5
all_words_contro=counted_words %>%
  filter(hashtags=="impeachment" | hashtags=="mattarelladimettiti" | hashtags =="impeachmentmattarella" & n>=minimum_repetitions)
  #filter(hashtags=="mattarella" & n>=100)
all_words_pro=counted_words %>%
  filter(hashtags=="iostoconmattarella"& n>=minimum_repetitions )
only_pro = anti_join(all_words_pro,all_words_contro,by="word")
only_contra = anti_join(all_words_contro,all_words_pro,by="word")
common=inner_join(all_words_pro,all_words_contro,by="word")
#plot the top 20 words
plot1=only_pro[1:20,] %>%
  #count(word, sort = TRUE) %>%
  #top_n(20,n) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x = word, y = n)) +
  geom_col(aes(alpha=n),show.legend =FALSE) +
  xlab(NULL) +
  coord_flip() +
  labs(x = "Count",
       y = "Unique words",
       title = " Pro Mattarella group",
       subtitle = "Hashtags: #iostoconmattarella",
       
       caption=paste("\nN° of words:",as.character(dim(only_pro)),"out of",as.character(dim(all_words_pro)) ))+
  theme_minimal()

#plot the top 20 words
plot2=only_contra[1:20,] %>%
  #count(word, sort = TRUE) %>%
  #top_n(20,n) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x = word, y = n)) +
  geom_col(aes(alpha=n),show.legend =FALSE) +
  xlab(NULL) +
  coord_flip() +
  labs(x = "Count",
       y = "Unique words",
       title = "Contra mattarella group ",
       subtitle = "  Hashtags: #impeachment,\n #mattarellaimpeachment,  #mattarelladimettiti",
       caption=paste("\nN° of words:",as.character(dim(only_contra)),"out of",as.character(dim(all_words_contro)),"filter=",as.character(minimum_repetitions) ))+
  
  theme_minimal()

grid.arrange(plot1, plot2, ncol=2,top = "Lexicon comparison betweet pro and contra Mattarella groups")


#-------------------------------------------
```

The situation depicted in this plot in my opinion is quite clear.
The words used by the two groups are quite different, so it means that my system works.
The first group is characterized by two trends: it defends the President ("vicinanza","saggezza"), and is mainly scared by the ones attacching him:("becero","razzismi","violenti","populismi").
The second group is clearly against.
Of course this result could look quite boring, since I choose "ad hoc" hashtags with a quite precise meaning. Let's look at a more interesting example: a confrontation between Salvini and Di Maio, both principal actors in this story:

```{r "salvini vs dimaio"}
minimum_repetitions=20
hashtag_contro="dimaio"
hashtag_pro="salvini"
#and now a comparison of the vocabulary used by the subgroups
#----------------------------------------
#Common and special lexicons

all_words_contro=counted_words %>%
  filter(hashtags==hashtag_contro& n>=minimum_repetitions)
  #filter(hashtags=="mattarella" & n>=100)
all_words_pro=counted_words %>%
  filter(hashtags==hashtag_pro& n>=minimum_repetitions )
only_pro = anti_join(all_words_pro,all_words_contro,by="word")
only_contra = anti_join(all_words_contro,all_words_pro,by="word")
common=inner_join(all_words_pro,all_words_contro,by="word")
#plot the top 20 words
plot1=only_pro[1:20,] %>%
  #count(word, sort = TRUE) %>%
  #top_n(20,n) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x = word, y = n)) +
  geom_col(aes(alpha=n),show.legend =FALSE) +
  xlab(NULL) +
  coord_flip() +
  labs(x = "",
       y = "",
       title = hashtag_pro,
       subtitle = paste("Hashtags:#",hashtag_pro),
       
       caption=paste("\nN° of words:",as.character(dim(only_pro)),"out of",as.character(dim(all_words_pro)),"filter=",as.character(minimum_repetitions) ))+
  theme_minimal()

#plot the top 20 words
plot2=only_contra[1:20,] %>%
  #count(word, sort = TRUE) %>%
  #top_n(20,n) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x = word, y = n)) +
  geom_col(aes(alpha=n),show.legend =FALSE) +
  xlab(NULL) +
  coord_flip() +
  labs(x = "",
       y = "",
       title = hashtag_contro,
       subtitle = paste("Hashtags:#",hashtag_contro),
       caption=paste("\nN° of words:",as.character(dim(only_contra)),"out of",as.character(dim(all_words_contro)),"filter=",as.character(minimum_repetitions) ))+
  
  theme_minimal()

grid.arrange(plot1, plot2, ncol=2,top = paste( hashtag_pro," vs ",hashtag_contro ))
```


In my opinion it's clear who is the political winner here, the words associated with Salvini are in majority his words, "clandestini", "sicurezza", "immigrati", are all words strongly rooted in his political propaganda machine. Meanwhile words associated to Di Maio are often negative ones like "bibitaro", "smentito", "giravolte", "barzelletta".











##Correlation between terms
Until now I worked at the level of single words, here I want to discuss at the level of groups of words using the correlation between them. Details on how the correlation coefficient is computed can be found here [4], and here for a theoretical explanation [5].
I cleaned the data in such a way that a n-gram analysis would be misleading, since I stripped the tweets from all the hashtags, and people usually refer to other people using them. For the correlation analysis this is not a problem, since date are grouped by hashtags, so the interpretation that  I give to the results are in therm of pairwise correlation in relation to a given hashtag. Since the goal of the project is to study the sentiment associated to a given political person or topic, that's exactly what we want.
The data are plotted using a network of words, linked between them by their correlation coefficient. In such a way it's possible to see that some pattern of words were used very often.
The real power of this approach come when it's applied interactively, so I developed an application that permits to perform this task filtering for the minimum number of repetitions and a minimum value of the threshold.



Link for the interactive application: https://offlagadiscopax.shinyapps.io/interactive_correlation/

Unfortunately it's not possible to deploy the online version with a key component, present on my local version, that is a search area where is possible to find all the tweets given some key words.
That's extremely useful since combines in a fluent and easy way the data mining component, that is finding common cluster of words, and the analytical skills of the user that can search between the raw text of the tweets to understand in which way those words were used. In my opinion this approach is the best one since even state of the art machine learning systems can't understand human irony, quite common on this dataset of tweets. I was not able to understand way the online version has this problem, but I decided to publish it anyway since it's one of the main component of my work.

Here a static example:


```{r "mattarelladimettiti"}
hash="mattarelladimettiti"
filter=20
cor=0.4

 unique_tweets_filtered= 
    unique_tweets_tidy%>%
    filter(hashtags==hash)
     
    
  
  data_prova=unique_tweets_filtered%>%
    select(status_id,stripped_text)
  
  
  data_prova_words=data_prova %>%
    unnest_tokens(word,stripped_text)
  
  
  # remove stop words from your list of words
  word_cors  <- 
    
    data_prova_words %>%
    anti_join(italian_stop_words) %>%
      group_by(word) %>%
      filter(n() >= filter) %>%
      pairwise_cor(word, status_id, sort = TRUE)
 
  
   set.seed(1)
      
      
      word_cors %>%
      filter(correlation > cor) %>%
      graph_from_data_frame() %>%
      ggraph(layout = "fr") +
      geom_edge_link(aes(edge_alpha = correlation), show.legend = TRUE,colour="red" ,width=1) +
      geom_node_point(color = "lightblue", size = 5) +
      geom_node_text(aes(label = name), repel = TRUE,color="black") +
  theme_void()+
        labs(title =paste( " Correlation between words, hashtag:",hash),
       caption=paste("threshold for correlation:",paste(as.character(cor)),"\nminimum number of repetitions:",as.character(filter)))



```


The dataset is filtered for the hashtag #mattarelladimettiti, and shows some quite interesting couples of words that reflects quite well the sentiment towards Mattarella.
In addition a cluster of words shows that the second part of the First Article of the Italian Constitution has been cited multiple times, in fact the cited phrase is :"La sovranità appartiene al popolo, che la esercita nelle forme e nei limiti della Costituzione."
At least that is what one would think without checking the original tweets, using the interactive app I discovered that actually this article has been cited changing some words, obtaining a completely different meaning.
Some of the tweets found:

" L'Italia è una Repubblica Presidenziale fondata sull'euro. La sovranità popolare appartiene alla BCE che la esercita nei limiti dettati da Berlino. #mattarelladimettiti #Costituzione #euro"

"L'Italia è una Repubblica NON democratica, fondata sui MERCATI. La sovranità appartiene al PRESIDENTE DELLA REPUBBLICA, che la esercita nelle forme e nei limiti dei MERCATI "



Those are good examples of why I think that the human understanding component is still irreplaceable for this kind of analysis.










#Retweets analysis
##Disclaimer
There were a problem during the data gathering phase, so I don't have all the retweets with hashtag #mattarella.
Unfortunatly when I realized that it was to late to re-collect them, because of free Twitter API's temporal restrictions.
So for those tweets I couldn't perform all the analysis that follows.


##Top three Retweets
The three most retweeted statuses has been:

1) "Dignità, forza, difesa interesse nazionale e Costituzione. Il discorso del Presidente #Mattarella rappresenta l’Italia al suo meglio. L’aggressivita’ scomposta e infantile degli sproloqui di Salvini e Di Maio l’opposto. Rimaniamo uniti a difesa del paese e delle sue istituzioni. " from Carlo Calenda, from PD political party, with 4213 retweets

2) "#Quirinale: dichiarazioni alla stampa del Presidente della Repubblica Sergio #Mattarella al termine delle #consultazioni https://t.co/0pPcPZewwT " from the official twitter account "Quirinale", with 4175 retweets and attached a link with the live video straming of the President Sergio Mattarella talking about his veto about Savona.

2) "E' certo che il governo Cottarelli non avrà la fiducia delle Camere. Per la prima volta nella storia avremo un governo che non ha il sostegno del popolo nè del Parlamento. E' vergognoso: sarà un governo anti-italiano di occupazione delle istituzioni. Voto subito! #IlMioVotoConta " with 3864 retweets from Luigi Di Maio, M5S Political leader.


##Penetration power
In this section retweets are analyzied from a network point of view.
This part of the work present some novelty too, since I did't found any similar approach on leterature, but again probably somebody did this kind of analysis but I was not able to find it.
I defined a quantity called "penetration power", that is the cumulative sum of all the followers of users that retweeted a given status.
This is an upper bound of the maximum number of people that saw a given status. It's an upper bound for two reasons: the first is that it counts multiple times common followers, the second is that at a given moment of time I don't think that all the followers get the messages from all their friends (people that are followed by them). Neverthless I think it's an interesting estimate to grasp the order of magnitude of people implicated in the political conversation on the social network, in respect to the total population.


```{r "Penetration power"}
#analisi retweets
library(tidyverse)
library(rtweet)
#How many people the most retweeted tweet reached? pay attention that followers could be replicated.
dataset=readRDS("~/0dssc/data_managment/esame_finale/data/FINAL_DATA_DEFINITIVO/tutti_i_tweet_no_duplicati.RDS" )
dataset=dataset%>%filter(lang=="it")


#find top retweets
top_retweets=dataset%>%
  filter(is_retweet==FALSE)%>%
  top_n(200,retweet_count)%>%
  arrange(-retweet_count)
retweetters=readRDS("~/0dssc/data_managment/esame_finale/data/info_users/all_users_info.rds")
#find all the retweets of given tweet
retweet=filter(dataset,retweet_status_id==top_retweets$status_id[14])



full_data=inner_join(retweet,retweetters,by="user_id")

full_data=full_data%>%
  arrange(created_at)%>%
  mutate(penetrazione_followers=cumsum(followers_count))%>%
  mutate(penetrazione_friends=cumsum(friends_count))


retweet=filter(dataset,retweet_status_id==top_retweets$status_id[3])
full_data=inner_join(retweet,retweetters,by="user_id")
full_data=full_data%>%
  arrange(created_at)%>%
  mutate(penetrazione_followers=cumsum(followers_count))%>%
  mutate(penetrazione_friends=cumsum(friends_count))
penetration_followers=select(full_data,created_at,penetrazione_followers,penetrazione_friends)
retweet_3=mutate(retweet,autore=as.factor("top3"))
penetration_followers_3=mutate(penetration_followers,autore=as.factor("top3"))


retweet=filter(dataset,retweet_status_id==top_retweets$status_id[8])
full_data=inner_join(retweet,retweetters,by="user_id")
full_data=full_data%>%
  arrange(created_at)%>%
  mutate(penetrazione_followers=cumsum(followers_count))%>%
  mutate(penetrazione_friends=cumsum(friends_count))
penetration_followers=select(full_data,created_at,penetrazione_followers,penetrazione_friends)
retweet_8=mutate(retweet,autore=as.factor("top8"))
penetration_followers_8=mutate(penetration_followers,autore=as.factor("top8"))


retweet=filter(dataset,retweet_status_id==top_retweets$status_id[11])
full_data=inner_join(retweet,retweetters,by="user_id")
full_data=full_data%>%
  arrange(created_at)%>%
  mutate(penetrazione_followers=cumsum(followers_count))%>%
  mutate(penetrazione_friends=cumsum(friends_count))
penetration_followers=select(full_data,created_at,penetrazione_followers,penetrazione_friends)
retweet_11=mutate(retweet,autore=as.factor("top11"))
penetration_followers_11=mutate(penetration_followers,autore=as.factor("top11"))


retweet=filter(dataset,retweet_status_id==top_retweets$status_id[14])
full_data=inner_join(retweet,retweetters,by="user_id")
full_data=full_data%>%
  arrange(created_at)%>%
  mutate(penetrazione_followers=cumsum(followers_count))%>%
  mutate(penetrazione_friends=cumsum(friends_count))
penetration_followers=select(full_data,created_at,penetrazione_followers,penetrazione_friends)
retweet_14=mutate(retweet,autore=as.factor("top14"))
penetration_followers_14=mutate(penetration_followers,autore=as.factor("top14"))

retweet=rbind(retweet_3,retweet_8,retweet_11,retweet_14)
penetration_followers=rbind(penetration_followers_3,penetration_followers_8,penetration_followers_11,penetration_followers_14)


penetration_followers%>%
  #filter(autore=="top14")%>%
ggplot( aes(created_at, penetrazione_followers,color=autore) )+
  geom_line() +
 labs(
   x = NULL, y = NULL,
  title="Number of reached followers",
  color = "Ranking ")+
  theme_minimal()
```

For the problem highlighted in the disclaimer I am not able to compute the penetration power for all the retweets, so I plotted only the four most retweetted statuses with complete data aviable.

The status with the biggest penetration power is the one from Di Maio, and it reached 4 million users in less than a day.
The other three are:

top8: "Mettete una bandiera tricolore alla vostra finestra #IlMioVotoConta https://t.co/4CZtJbonzM https://t.co/Vr5KCBSxXY", still Di Maio.

top11: "Pertini disse no a Cossiga su Darida; Scalfaro a Berlusconi su Previti; Ciampi a Berlusconi su Maroni; Napolitano a Renzi su Gratteri. È l'articolo 92 della Costituzione. Ignorarlo e bullizzare il Capo dello Stato è una prepotenza grave e incostituzionale. #iostoconMattarella " from user Dario Parrini, PD party

top14: "Di Maio è l'unico napoletano che è riuscito a farsi fare il gioco delle tre carte da un milanese. #IlMioVotoConta" from IL__Mascetti, an anonimous user that made this joke that reached more than a million and a half accounts.

I would like to stress the fact that since I lack data for #mattarella this part of analysis is incomplete, and is here only as a reference.
Neverthless if we think that the total amount of voters during the political elections of 2018 is about 30 million [6], we can conclude that the discussion on social networks can have an influence on a good amount of voters.







#Notes on the interactive apps
The apps are hosted on shinyapps.io that limit the number of hosted apps to 5 and the total number apps use to hours to 24 per month, so they could be unreachable, depending on usage.
They are developed having in mind a personal use, so they will not accept all the values, for example because the computations becomes prohibitive. 

#Bibliography and downloads links
[1] https://www.sciencedirect.com/science/article/pii/S0378873316304166

[2] "https://raw.githubusercontent.com/stopwords-iso/stopwords-it/master/stopwords-it.txt"

[3] https://www.ansa.it/sito/notizie/politica/2018/05/30/governo-lega-e-m5s-ci-riprovano_78fb9b46-a537-417b-af3f-6465200bb64d.html

[4] https://cran.r-project.org/web/packages/widyr/widyr.pdf

[5] https://en.wikipedia.org/wiki/Phi_coefficient

[6] https://en.wikipedia.org/wiki/Italian_general_election,_2018

dataset download link (from my personal MEGA account): https://mega.nz/#F!YZxDCLIT!il8RD2OMymMyM7TW8ABOUQ












